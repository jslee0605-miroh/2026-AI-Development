\documentclass[aspectratio=169]{beamer}
\usetheme{Madrid}
\usecolortheme{default}
\usepackage{tikz}
\usetikzlibrary{calc}

\title{DSI: AI Development Program}
\subtitle{Part 1: Introduction to AI }
\author{University of Chicago}
\date{\today}

\begin{document}

\frame{\titlepage}

% Basics of building AI Systems
\section{Introduction and Overview}

\begin{frame}
	\begin{itemize}
		\item Who am I
		\item Learning Objectives
        \item Basics and Terminology
        \item Basics of Building		
	\end{itemize}
\end{frame}


\begin{frame}{Who am I}
\begin{itemize}
\item Director Data Science Clinic
\item Undergrad: UC Berkeley, PhD: UCLA
\item Worked in Consulting, Video Games, AI
\end{itemize}
\end{frame}

\begin{frame}{Some Games I've Worked on}
% Images live in: lecture_1/slides/images/
% Collage layout with intentional overlap (TikZ absolute positioning).
\begin{tikzpicture}[remember picture,overlay]
  \coordinate (NW) at (current page.north west);

  % NOTE: Use a consistent anchor everywhere (north west) and position by offsets.
  % The order matters (later nodes sit on top of earlier ones).


  \node[anchor=north west] at ($(NW)+(0.04\paperwidth,-0.1\paperheight)$) {%
    \IfFileExists{images/kovaak.jpg}{\includegraphics[width=0.46\paperwidth]{images/kovaak.jpg}}{}%
  };

  \node[anchor=north west] at ($(NW)+(0.48\paperwidth,-0.1\paperheight)$) {%
    \IfFileExists{images/familyguyquestforstuff.png}{\includegraphics[width=0.34\paperwidth]{images/familyguyquestforstuff.png}}{}%
  };


  \node[anchor=north west] at ($(NW)+(0.02\paperwidth,-0.4\paperheight)$) {%
    \IfFileExists{images/cityrush.jpg}{\includegraphics[width=0.28\paperwidth]{images/cityrush.jpg}}{}%
  };


  \node[anchor=north west] at ($(NW)+(0.49\paperwidth,-0.435\paperheight)$) {%
    \IfFileExists{images/spellstorm.jpg}{\includegraphics[width=0.28\paperwidth]{images/spellstorm.jpg}}{}%
  };



  \node[anchor=north west] at ($(NW)+(0.3\paperwidth,-0.4\paperheight)$) {%
    \IfFileExists{images/wofp.jpg}{\includegraphics[width=0.2\paperwidth]{images/wofp.jpg}}{}%
  };

  \node[anchor=north west] at ($(NW)+(0.8\paperwidth,-0.1\paperheight)$) {%
    \IfFileExists{images/sonicdash.jpg}{\includegraphics[width=0.2\paperwidth]{images/sonicdash.jpg}}{}%
  };



  \node[anchor=north west] at ($(NW)+(0.80\paperwidth,-0.72\paperheight)$) {%
    \IfFileExists{images/vippoker.jpg}{\includegraphics[width=0.13\paperwidth]{images/vippoker.jpg}}{}%
  };

  \node[anchor=north west] at ($(NW)+(0.75\paperwidth,-0.45\paperheight)$) {%
    \IfFileExists{images/tinyzoo_friends.jpg}{\includegraphics[width=0.22\paperwidth]{images/tinyzoo_friends.jpg}}{}%
  };



  \node[anchor=north west] at ($(NW)+(0.23\paperwidth,-0.76\paperheight)$) {%
    \IfFileExists{images/sonicjumpfever.jpg}{\includegraphics[width=0.1\paperwidth]{images/sonicjumpfever.jpg}}{}%
  };

  \node[anchor=north west] at ($(NW)+(0.50\paperwidth,-0.7\paperheight)$) {%
    \IfFileExists{images/TinyMonsters.png}{\includegraphics[width=0.26\paperwidth]{images/TinyMonsters.png}}{}%
  };
  
\end{tikzpicture}
\end{frame}
    




\section{Part 1: Large Language Models (LLMs)}

\begin{frame}{What is an LLM?}
\begin{itemize}
    \item A \textbf{Large Language Model (LLM)} is an artificial intelligence system trained on massive amounts of text data (often trillions of words)
    \item These models learn patterns in language and can:
    \begin{itemize}
        \item Generate human-like text
        \item Answer questions
        \item Translate languages
        \item Summarize documents
        \item Write code
        \item And much more
    \end{itemize}
    \item Think of an LLM as a very sophisticated autocomplete system that has read a significant portion of the internet
\end{itemize}
\end{frame}

\begin{frame}{Foundation Models}
\begin{itemize}
    \item A \textbf{foundation model} is a large-scale machine learning model trained on broad data (generally using self-supervision at scale) that can be adapted to a wide range of downstream tasks
    \item \textbf{Key characteristics:}
    \begin{itemize}
        \item Broad training: Trained on diverse, large-scale datasets
        \item General-purpose: Can be adapted to many different tasks
        \item Transfer learning: Knowledge learned from training can be applied to new tasks
        \item Base for specialization: Can be fine-tuned or used as-is for specific applications
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Foundation Models vs LLMs}
\begin{itemize}
    \item \textbf{Foundation Model} is the broader categoryâ€”it includes models for vision, audio, code, etc.
    \item \textbf{LLM (Large Language Model)} is a type of foundation model focused specifically on language
    \item \textbf{Examples of foundation models:}
    \begin{itemize}
        \item \textbf{Language}: GPT-4, Claude, Llama (these are LLMs)
        \item \textbf{Vision}: CLIP, DALL-E
        \item \textbf{Code}: Codex, CodeLlama
        \item \textbf{Multimodal}: GPT-4V (vision + language), Gemini (multimodal)
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Key Terminology}
\begin{itemize}
    \item \textbf{Foundation Model}: A large-scale model trained on broad data that can be adapted to many tasks
    \item \textbf{LLM (Large Language Model)}: A type of foundation model specifically designed for language tasks
    \item \textbf{Token}: The basic unit of text that an LLM processes (word, part of word, or punctuation)
    \item \textbf{Prompt}: The input text you give to an LLM
    \item \textbf{Completion/Response}: The output text generated by the LLM
    \item \textbf{Context Window}: The maximum number of tokens an LLM can process in a single conversation
\end{itemize}
\end{frame}

\begin{frame}{Key Terminology (continued)}
\begin{itemize}
    \item \textbf{Temperature}: A parameter that controls randomness (Lower = more deterministic, Higher = more creative)
    \item \textbf{Fine-tuning}: Training an LLM on specific data to improve performance on particular tasks
    \item \textbf{Inference}: The process of generating text from an LLM (as opposed to training)
\end{itemize}
\end{frame}

\begin{frame}{Context windows: what they are (and why you care)}
\begin{itemize}
    \item The \textbf{context window} is the model's working memory for a single request
    \item Everything the model can use must fit inside it:
    \begin{itemize}
        \item system prompt + developer instructions
        \item user messages
        \item tool outputs / retrieved documents
        \item the model's own previous messages (in chat)
    \end{itemize}
    \item When you exceed the window, something gets dropped or truncated
\end{itemize}
\end{frame}

\begin{frame}{Context windows: practical implications}
\begin{itemize}
    \item \textbf{Tradeoffs}: more context can improve grounding, but increases cost/latency
    \item \textbf{Recency and placement matter}: newer text is usually attended to more
    \item \textbf{Failure modes} to watch for:
    \begin{itemize}
        \item forgetting earlier constraints
        \item losing critical details due to truncation
        \item quoting the wrong source when too many docs are included
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{System prompts: role and best practices}
\begin{itemize}
    \item The \textbf{system prompt} sets the highest-level behavior (tone, rules, safety, format)
    \item Use it to encode \textbf{non-negotiables}:
    \begin{itemize}
        \item output format requirements (e.g., JSON only)
        \item refusal and safety rules
        \item grounding requirements (cite sources; do not invent facts)
    \end{itemize}
    \item Keep it \textbf{short, explicit, testable}; avoid vague goals like ``be helpful''
\end{itemize}
\end{frame}

\begin{frame}{Context engineering (a.k.a. make the model successful)}
\begin{itemize}
    \item \textbf{Context engineering} = designing what goes into the context window so the model can reliably do the task
    \item Common building blocks:
    \begin{itemize}
        \item task definition + success criteria
        \item constraints and policies (what is allowed/not allowed)
        \item examples (few-shot) and edge cases
        \item retrieved evidence (RAG) with citations
        \item tool schemas for structured interaction with code
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{A simple context template (recommended)}
\begin{enumerate}
    \item \textbf{System}: rules, safety, output format
    \item \textbf{Developer}: task-specific policies (what to do, what not to do)
    \item \textbf{User}: the request + required inputs
    \item \textbf{Evidence}: only the relevant retrieved snippets (not entire documents)
    \item \textbf{Tools}: schemas + allowed actions
    \item \textbf{Final}: ``Return JSON only'' / ``Cite sources'' / ``Ask clarifying questions if needed''
\end{enumerate}
\end{frame}

\begin{frame}{Example LLM Models: Commercial}
\begin{itemize}
    \item \textbf{GPT-4} (OpenAI)
    \begin{itemize}
        \item One of the most capable models
        \item Available via OpenAI API
        \item Powers ChatGPT Plus
    \end{itemize}
    \item \textbf{Claude 3.5 Sonnet} (Anthropic)
    \begin{itemize}
        \item Strong reasoning and coding capabilities
        \item Available via Anthropic API
    \end{itemize}
    \item \textbf{Gemini Pro} (Google)
    \begin{itemize}
        \item Google's flagship model
        \item Available via Google AI Studio
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Example LLM Models: Open Source}
\begin{itemize}
    \item \textbf{Hugging Face} is a platform hosting thousands of open-source models
    \item Popular models include:
    \begin{itemize}
        \item \textbf{Llama 3} (Meta)
        \item \textbf{Mistral} (Mistral AI)
        \item \textbf{Phi} (Microsoft)
        \item \textbf{Gemma} (Google)
    \end{itemize}
    \item You can use these models via:
    \begin{itemize}
        \item Hugging Face Transformers library (Python)
        \item Hugging Face Inference API
        \item Local deployment
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Model Aggregators}
\begin{itemize}
    \item \textbf{Open Router} provides access to many models through a single API
    \begin{itemize}
        \item Unified API for 100+ models
        \item Compare models side-by-side
        \item Pay-per-use pricing
        \item Easy to switch between models
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Token economics: why tokens matter}
\begin{itemize}
    \item Most LLM APIs charge by tokens:
    \begin{itemize}
        \item \textbf{input tokens} (prompt, system prompt, retrieved docs, tool outputs)
        \item \textbf{output tokens} (the model's response)
    \end{itemize}
    \item Cost and latency generally scale with total tokens processed
    \item \textbf{Context engineering is cost engineering}:
    \begin{itemize}
        \item tighter prompts $\rightarrow$ lower cost and often higher reliability
        \item smaller context when possible $\rightarrow$ faster, cheaper
    \end{itemize}
\end{itemize}
\vspace{0.2cm}
\footnotesize Pricing references: \url{https://openai.com/api/pricing/}
\end{frame}

\begin{frame}{Cost math (simple model)}
\begin{itemize}
    \item Typical pricing is ``\$ per 1M tokens''
    \item Approximate cost per call:
    \[
      \text{cost} \approx \left(\frac{T_{in}}{10^6} \cdot p_{in}\right) + \left(\frac{T_{out}}{10^6} \cdot p_{out}\right)
    \]
    \item Engineering levers:
    \begin{itemize}
        \item reduce retrieved context (better retrieval, smaller snippets)
        \item constrain output length (max tokens, structured output)
        \item pick the cheapest model that meets quality requirements
        \item cache repeated context (where supported)
    \end{itemize}
\end{itemize}
\vspace{0.2cm}
\footnotesize Pricing references: \url{https://openai.com/api/pricing/}
\end{frame}

\begin{frame}{Real-world pricing examples (text)}
\textbf{Example (OpenAI GPT-4.1 family; per 1M tokens)}\vspace{0.2cm}

\begin{tabular}{|l|r|r|}
\hline
\textbf{Model} & \textbf{Input} & \textbf{Output} \\
\hline
GPT-4.1 & \$2.00 & \$8.00 \\
GPT-4.1 mini & \$0.40 & \$1.60 \\
GPT-4.1 nano & \$0.10 & \$0.40 \\
\hline
\end{tabular}

\vspace{0.4cm}
\begin{itemize}
    \item Decision implication: use smaller/cheaper models for simpler tasks (classification, routing, QC), reserve larger models for harder steps
\end{itemize}
\vspace{0.2cm}
\footnotesize Source: \url{https://openai.com/index/gpt-4-1/}
\end{frame}

\begin{frame}{Multimodal pricing: what changes?}
\begin{itemize}
    \item Multimodal models may price \textbf{each modality differently}:
    \begin{itemize}
        \item text tokens (input/output)
        \item image generation (often priced by image or image tokens)
        \item audio tokens (speech in/out) and realtime usage
        \item video generation (often priced per second/resolution)
    \end{itemize}
    \item Decision implication:
    \begin{itemize}
        \item use multimodal only when it adds value (OCR, visual QA, audio transcription)
        \item consider hybrid pipelines (cheap OCR $\rightarrow$ text-only LLM) to control cost
    \end{itemize}
\end{itemize}
\vspace{0.2cm}
\footnotesize Pricing references: \url{https://openai.com/api/pricing/} \; and \url{https://platform.openai.com/docs/pricing}
\end{frame}

\begin{frame}{LLM Limitations}
While LLMs are powerful, they have important limitations:
\begin{enumerate}
    \item \textbf{Knowledge Cutoff}: They only know information from their training data up to a certain date
    \item \textbf{Hallucination}: They can generate plausible-sounding but incorrect information
    \item \textbf{No Real-World Actions}: They can't directly interact with external systems (databases, APIs, file systems)
    \item \textbf{Context Limits}: They have maximum context window sizes
    \item \textbf{Static Knowledge}: They can't learn new information after training without fine-tuning or retrieval
\end{enumerate}
\end{frame}

\section{Part 2: AI Agents}

\begin{frame}{What is an AI Agent?}
\begin{itemize}
    \item An \textbf{AI Agent} is an LLM that can use \textbf{tools} to interact with the outside world
    \item While a basic LLM can only generate text based on its training data, an agent can:
    \begin{itemize}
        \item Query databases
        \item Call APIs
        \item Read files
        \item Execute code
        \item Search the web
        \item Perform actions in real-time
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{LLM vs Agent: Key Differences}
\begin{table}
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Aspect} & \textbf{LLM} & \textbf{Agent} \\
\hline
Capabilities & Text generation only & Text + tool usage \\
\hline
Knowledge & Training data only & Training data + real-time data \\
\hline
Actions & None & Can perform actions via tools \\
\hline
Interactivity & One-shot responses & Can loop and iterate \\
\hline
\end{tabular}
\end{table}
\end{frame}

\begin{frame}{The Agent Loop}
\begin{center}
\Large
Observe $\rightarrow$ Think $\rightarrow$ Act $\rightarrow$ Observe $\rightarrow$ \ldots
\end{center}
\vspace{1cm}
\begin{enumerate}
    \item \textbf{Observe}: Receive input (user query, tool results, system state)
    \item \textbf{Think}: Process information and decide what to do next
    \item \textbf{Act}: Execute actions (call tools, generate responses)
    \item \textbf{Observe}: See the results and continue the loop
\end{enumerate}
\end{frame}

\begin{frame}{Example Agent Systems}
\begin{itemize}
    \item \textbf{LangChain Agents}
    \begin{itemize}
        \item Framework for building LLM applications
        \item Supports multiple LLM providers
        \item Tool integration and agent orchestration
    \end{itemize}
    \item \textbf{Claude with MCP} (Model Context Protocol)
    \begin{itemize}
        \item Claude Desktop can connect to MCP servers
        \item Access custom tools and data sources
    \end{itemize}
    \item \textbf{ChatGPT with Plugins/Code Interpreter}
    \item \textbf{Cursor AI}
\end{itemize}
\end{frame}

\section{Part 3: Tools}

\begin{frame}{What is a Tool?}
\begin{itemize}
    \item A \textbf{tool} is a function that an AI agent can call to interact with external systems
    \item Tools bridge the gap between the LLM's text generation capabilities and real-world actions
    \item Every tool has:
    \begin{enumerate}
        \item \textbf{Name}: A unique identifier (e.g., \texttt{get\_player\_list})
        \item \textbf{Description}: What the tool does and when to use it
        \item \textbf{Input Schema}: Parameters the tool accepts (JSON Schema format)
        \item \textbf{Implementation}: The actual code that executes when called
    \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}{Common Tool Categories}
\begin{itemize}
    \item \textbf{Database Tools}: Query databases, insert/update/delete records
    \item \textbf{API Tools}: Call REST APIs, interact with web services
    \item \textbf{File System Tools}: Read/write files, list directories
    \item \textbf{Code Execution Tools}: Run Python code, execute shell commands
    \item \textbf{Web Tools}: Search the web, scrape websites
\end{itemize}
\end{frame}

\section{Part 4: Model Context Protocol (MCP)}

\begin{frame}{What is MCP?}
\begin{itemize}
    \item The \textbf{Model Context Protocol (MCP)} is an open protocol created by Anthropic
    \item Enables AI assistants to securely connect to external tools and data sources
    \item Provides a standardized way for AI assistants to discover and use capabilities from external systems
\end{itemize}
\end{frame}

\begin{frame}{Why MCP?}
\begin{itemize}
    \item Before MCP, each AI assistant had its own way of connecting to external tools:
    \begin{itemize}
        \item ChatGPT had plugins
        \item Claude had custom integrations
        \item Each system was proprietary
    \end{itemize}
    \item MCP provides:
    \begin{itemize}
        \item \textbf{Standardization}: One protocol works across multiple AI assistants
        \item \textbf{Security}: Secure, controlled access to tools
        \item \textbf{Discoverability}: AI assistants can discover available tools automatically
        \item \textbf{Composability}: Tools can be combined and chained together
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{MCP Architecture}
\begin{center}
\Large
AI Assistant (MCP Client) \\
$\downarrow$ \\
MCP Protocol \\
$\downarrow$ \\
MCP Server \\
$\downarrow$ \\
Your Application (Flask API, Database, File System)
\end{center}
\end{frame}

\begin{frame}{Key MCP Components}
\begin{enumerate}
    \item \textbf{MCP Server}: Exposes capabilities (tools, resources, prompts) to AI assistants
    \item \textbf{MCP Client}: AI assistant that connects to servers (Claude Desktop, Cursor, etc.)
    \item \textbf{Tools}: Functions that an AI can call to interact with external systems
    \item \textbf{Resources}: Data sources that an AI can read (files, database tables, API endpoints)
    \item \textbf{Prompts}: Pre-defined prompt templates for common tasks
\end{enumerate}
\end{frame}

\begin{frame}{How MCP Works}
\begin{enumerate}
    \item User asks: "Get me all players"
    \item MCP Client analyzes query and selects appropriate tool
    \item Client calls tool: \texttt{get\_player\_list()}
    \item MCP Server makes HTTP request to Flask API
    \item API returns player data
    \item Server returns tool result to client
    \item Client incorporates result into response
    \item User receives: "Here are all the players: \ldots"
\end{enumerate}
\end{frame}

\begin{frame}{Why not a REST API?}
\begin{itemize}
    \item MCP uses \textbf{Server-Sent Events (SSE)} for real-time communication
    \item \textbf{REST}: One request $\rightarrow$ wait $\rightarrow$ one response $\rightarrow$ connection closes
    \item \textbf{MCP/SSE}: Long-lived connection $\rightarrow$ multiple messages $\rightarrow$ streaming updates
    \item This enables:
    \begin{itemize}
        \item Real-time feedback during tool execution
        \item Streaming responses for long operations
        \item Continuous communication between client and server
    \end{itemize}
\end{itemize}
\end{frame}

\section{Part 5: Claude Skills}

\begin{frame}{What are Claude Skills?}
\begin{itemize}
    \item \textbf{Claude Skills} (also called "Actions" or "Tool Use") is Claude's built-in capability to use tools
    \item When you give Claude access to tools, it can:
    \begin{itemize}
        \item Automatically decide when to use tools
        \item Call multiple tools in sequence
        \item Combine tool results into coherent responses
    \end{itemize}
    \item Claude Skills work with:
    \begin{itemize}
        \item MCP servers (via Claude Desktop)
        \item Custom API integrations
        \item Function calling (via Anthropic API)
    \end{itemize}
\end{itemize}
\end{frame}

\section{Part 6: How Everything Works Together}

\begin{frame}{Complete System Architecture}
\begin{center}
\Large
User Interface \\
$\downarrow$ \\
AI Assistant Layer (Claude Desktop) \\
$\downarrow$ \\
MCP Protocol Layer \\
$\downarrow$ \\
MCP Server Layer \\
$\downarrow$ \\
Application Layer (Flask API, Database, File System)
\end{center}
\end{frame}

\begin{frame}{Example: Complete Interaction Flow}
\textbf{Scenario}: User asks "What colleges do players from Washington come from?"
\begin{enumerate}
    \item Claude analyzes query, identifies need for player data
    \item Selects \texttt{get\_players} tool
    \item Calls \texttt{get\_players(team="WAS")}
    \item MCP Server makes GET request to Flask API
    \item API queries database
    \item Returns player records
    \item Claude processes data, extracts unique colleges
    \item Returns formatted response to user
\end{enumerate}
\end{frame}

\begin{frame}{Summary}
\begin{itemize}
    \item \textbf{LLMs} are powerful text generation systems trained on massive datasets
    \item \textbf{AI Agents} extend LLMs with tool-using capabilities
    \item \textbf{Tools} bridge the gap between LLMs and real-world systems
    \item \textbf{MCP} provides a standardized protocol for connecting AI assistants to external tools
    \item Together, these technologies enable AI systems that can interact with the real world
\end{itemize}
\end{frame}

\begin{frame}{Questions?}
\begin{center}
\Large
Thank you!
\end{center}
\end{frame}

\end{document}

