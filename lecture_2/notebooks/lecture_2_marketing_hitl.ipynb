{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 2 — Marketing Automation with HITL\n",
        "\n",
        "**Goal**: Build a reliable outreach pipeline:\n",
        "\n",
        "1. Summarize lead context → **JSON**\n",
        "2. Draft outreach email + subject → **JSON**\n",
        "3. Run a QC pass that flags risky outputs\n",
        "4. Create a simple **human-in-the-loop** review queue\n",
        "5. Run a tiny evaluation to compare prompt versions\n",
        "\n",
        "## Setup\n",
        "This notebook expects an OpenRouter key:\n",
        "\n",
        "- `OPENROUTER_API_KEY` (required)\n",
        "- `OPENROUTER_MODEL` (optional; default set below)\n",
        "\n",
        "You will be provided keys in class. Do **not** commit keys to git.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import httpx\n",
        "import pandas as pd\n",
        "\n",
        "DATA_DIR = Path(\"../data\")\n",
        "OUTPUT_DIR = DATA_DIR / \"outputs\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "OPENROUTER_MODEL = os.getenv(\"OPENROUTER_MODEL\", \"openai/gpt-4o-mini\")\n",
        "\n",
        "if not OPENROUTER_API_KEY:\n",
        "    raise RuntimeError(\n",
        "        \"Missing OPENROUTER_API_KEY. Set it in your environment before running this notebook.\"\n",
        "    )\n",
        "\n",
        "print(\"Using model:\", OPENROUTER_MODEL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def openrouter_chat(messages: List[Dict[str, str]], *, temperature: float = 0.2) -> str:\n",
        "    \"\"\"Call OpenRouter Chat Completions API and return the assistant text.\"\"\"\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": OPENROUTER_MODEL,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": temperature,\n",
        "    }\n",
        "\n",
        "    with httpx.Client(timeout=60) as client:\n",
        "        resp = client.post(url, headers=headers, json=payload)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "\n",
        "    return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "def parse_json_strict(text: str) -> Dict[str, Any]:\n",
        "    \"\"\"Parse JSON; raise with a useful message if it fails.\"\"\"\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(f\"Model returned invalid JSON: {e}\\n---\\n{text}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "leads = pd.read_csv(DATA_DIR / \"leads.csv\")\n",
        "leads.head(3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "brand = (DATA_DIR / \"brand_guidelines.md\").read_text()\n",
        "one_pager = (DATA_DIR / \"product_one_pager.md\").read_text()\n",
        "rubric = (DATA_DIR / \"rubric.md\").read_text()\n",
        "\n",
        "print(brand.splitlines()[0:6])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 — Lead summarization (structured JSON)\n",
        "\n",
        "Edit the prompt to improve:\n",
        "- faithfulness to the lead notes\n",
        "- extracting concrete pain points\n",
        "- listing missing info questions that would help outreach\n",
        "\n",
        "**Constraint**: return *only* JSON.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LEAD_SUMMARY_SCHEMA = {\n",
        "    \"lead_summary\": \"string\",\n",
        "    \"pain_points\": [\"string\"],\n",
        "    \"suggested_angle\": \"string\",\n",
        "    \"missing_info\": [\"string\"],\n",
        "}\n",
        "\n",
        "\n",
        "def summarize_lead(row: pd.Series) -> Dict[str, Any]:\n",
        "    system = \"\"\"You are a careful assistant.\n",
        "Return ONLY valid JSON. No markdown. No backticks.\n",
        "\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Summarize the lead for outreach drafting.\n",
        "\n",
        "JSON schema (keys and types):\n",
        "{json.dumps(LEAD_SUMMARY_SCHEMA, indent=2)}\n",
        "\n",
        "Lead:\n",
        "- name: {row['name']}\n",
        "- role: {row['role']}\n",
        "- company: {row['company']}\n",
        "- industry: {row['industry']}\n",
        "- company_size: {row['company_size']}\n",
        "- region: {row['region']}\n",
        "- notes: {row['notes']}\n",
        "\n",
        "Rules:\n",
        "- Do not invent facts.\n",
        "- Pain points must be grounded in the notes.\n",
        "- Missing info must be phrased as questions.\n",
        "\"\"\"\n",
        "\n",
        "    text = openrouter_chat(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "    )\n",
        "    obj = parse_json_strict(text)\n",
        "    return obj\n",
        "\n",
        "\n",
        "example_summary = summarize_lead(leads.iloc[0])\n",
        "example_summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 — Draft outreach email (structured JSON)\n",
        "\n",
        "Use the lead summary + the allowed product facts + brand guidelines.\n",
        "\n",
        "**Constraints** (from the brand doc):\n",
        "- Subject: 4–8 words, no emojis\n",
        "- Body: 90–140 words\n",
        "- No hype, no guarantees, no advice\n",
        "\n",
        "Edit the prompt to improve quality and reduce compliance risk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DRAFT_SCHEMA = {\n",
        "    \"subject\": \"string\",\n",
        "    \"email_body\": \"string\",\n",
        "    \"personalization_tokens\": [\"string\"],\n",
        "}\n",
        "\n",
        "\n",
        "def draft_email(row: pd.Series, summary: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    system = \"\"\"You write high-quality, compliant outreach.\n",
        "Return ONLY valid JSON. No markdown. No backticks.\n",
        "\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Write a first-touch outreach email.\n",
        "\n",
        "You may ONLY use facts from:\n",
        "1) The lead fields and notes\n",
        "2) The product one-pager\n",
        "\n",
        "Brand guidelines:\n",
        "{brand}\n",
        "\n",
        "Product one-pager:\n",
        "{one_pager}\n",
        "\n",
        "JSON schema:\n",
        "{json.dumps(DRAFT_SCHEMA, indent=2)}\n",
        "\n",
        "Lead:\n",
        "- name: {row['name']}\n",
        "- role: {row['role']}\n",
        "- company: {row['company']}\n",
        "- industry: {row['industry']}\n",
        "- notes: {row['notes']}\n",
        "- compliance_tags: {row['compliance_tags']}\n",
        "\n",
        "Lead summary:\n",
        "{json.dumps(summary, indent=2)}\n",
        "\n",
        "Hard rules:\n",
        "- Subject line must be 4–8 words, no emojis.\n",
        "- Email body must be 90–140 words.\n",
        "- Avoid guarantees and advice.\n",
        "- Do not invent metrics, customers, or outcomes.\n",
        "- End with a simple sign-off.\n",
        "\"\"\"\n",
        "\n",
        "    text = openrouter_chat(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        temperature=0.4,\n",
        "    )\n",
        "    return parse_json_strict(text)\n",
        "\n",
        "\n",
        "example_draft = draft_email(leads.iloc[0], example_summary)\n",
        "example_draft\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 — QC pass (risk scoring)\n",
        "\n",
        "We use a second pass to flag drafts that might be unsafe to ship.\n",
        "\n",
        "Edit the QC prompt to:\n",
        "- catch invented facts / outcomes\n",
        "- catch compliance issues based on tags\n",
        "- catch format violations (word count, subject length)\n",
        "\n",
        "Return JSON with a `risk_level` and `reasons`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "QC_SCHEMA = {\n",
        "    \"risk_level\": \"low|medium|high\",\n",
        "    \"reasons\": [\"string\"],\n",
        "    \"required_edits\": [\"string\"],\n",
        "}\n",
        "\n",
        "\n",
        "def word_count(text: str) -> int:\n",
        "    return len([w for w in text.split() if w.strip()])\n",
        "\n",
        "\n",
        "def qc_draft(row: pd.Series, summary: Dict[str, Any], draft: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    system = \"\"\"You are a strict QA reviewer.\n",
        "Return ONLY valid JSON. No markdown. No backticks.\n",
        "\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"You are reviewing an outreach email for compliance, correctness, and formatting.\n",
        "\n",
        "Allowed facts:\n",
        "- Lead fields and notes\n",
        "- Product one-pager\n",
        "\n",
        "Brand guidelines:\n",
        "{brand}\n",
        "\n",
        "Product one-pager:\n",
        "{one_pager}\n",
        "\n",
        "JSON schema:\n",
        "{json.dumps(QC_SCHEMA, indent=2)}\n",
        "\n",
        "Lead:\n",
        "{row.to_dict()}\n",
        "\n",
        "Lead summary:\n",
        "{summary}\n",
        "\n",
        "Draft:\n",
        "{draft}\n",
        "\n",
        "Checks:\n",
        "1) Invented facts: flag any claims not supported by lead notes or one-pager.\n",
        "2) Compliance tags: enforce the lead's compliance_tags.\n",
        "3) Formatting: subject 4–8 words, body 90–140 words.\n",
        "4) Tone: avoid hype, guarantees, advice.\n",
        "\n",
        "Decide risk_level:\n",
        "- low: safe to send\n",
        "- medium: needs edits but safe after fixes\n",
        "- high: should not send; major issues\n",
        "\n",
        "Provide concrete required_edits.\n",
        "\"\"\"\n",
        "\n",
        "    text = openrouter_chat(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "    )\n",
        "    result = parse_json_strict(text)\n",
        "\n",
        "    # Add a couple deterministic checks too\n",
        "    subject_wc = word_count(draft.get(\"subject\", \"\"))\n",
        "    body_wc = word_count(draft.get(\"email_body\", \"\"))\n",
        "    if not (4 <= subject_wc <= 8):\n",
        "        result.setdefault(\"reasons\", []).append(\n",
        "            f\"Subject word count out of range: {subject_wc}\"\n",
        "        )\n",
        "    if not (90 <= body_wc <= 140):\n",
        "        result.setdefault(\"reasons\", []).append(\n",
        "            f\"Body word count out of range: {body_wc}\"\n",
        "        )\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "example_qc = qc_draft(leads.iloc[0], example_summary, example_draft)\n",
        "example_qc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 — Human-in-the-loop queue\n",
        "\n",
        "We create a review table and simulate a simple approve/edit/reject loop.\n",
        "\n",
        "In class, you’ll implement one of:\n",
        "- a minimal terminal-like loop in the notebook, or\n",
        "- a simple `review_decision` column you fill manually.\n",
        "\n",
        "The key idea: **models draft; humans ship**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch run: summaries -> drafts -> QC\n",
        "\n",
        "rows: List[Dict[str, Any]] = []\n",
        "\n",
        "for _, r in leads.iterrows():\n",
        "    s = summarize_lead(r)\n",
        "    d = draft_email(r, s)\n",
        "    q = qc_draft(r, s, d)\n",
        "\n",
        "    rows.append(\n",
        "        {\n",
        "            \"lead_id\": r[\"lead_id\"],\n",
        "            \"name\": r[\"name\"],\n",
        "            \"company\": r[\"company\"],\n",
        "            \"compliance_tags\": r[\"compliance_tags\"],\n",
        "            \"subject\": d.get(\"subject\", \"\"),\n",
        "            \"email_body\": d.get(\"email_body\", \"\"),\n",
        "            \"risk_level\": q.get(\"risk_level\", \"unknown\"),\n",
        "            \"reasons\": \" | \".join(q.get(\"reasons\", [])),\n",
        "            \"required_edits\": \" | \".join(q.get(\"required_edits\", [])),\n",
        "        }\n",
        "    )\n",
        "\n",
        "qc_report = pd.DataFrame(rows)\n",
        "qc_report\n",
        "\n",
        "# Simple HITL: you (or students) fill decisions\n",
        "qc_report[\"review_decision\"] = \"\"  # approve | edit | reject\n",
        "qc_report[\"review_notes\"] = \"\"  # optional\n",
        "\n",
        "# Suggested default policy (students can change): auto-approve low risk, queue medium/high\n",
        "qc_report.loc[qc_report[\"risk_level\"] == \"low\", \"review_decision\"] = \"approve\"\n",
        "qc_report.loc[qc_report[\"risk_level\"].isin([\"medium\", \"high\"]), \"review_decision\"] = \"edit\"\n",
        "\n",
        "qc_report\n",
        "\n",
        "# Save outputs\n",
        "qc_path = OUTPUT_DIR / \"qc_report.csv\"\n",
        "qc_report.to_csv(qc_path, index=False)\n",
        "\n",
        "# Final drafts: approved only\n",
        "final_drafts = qc_report[qc_report[\"review_decision\"] == \"approve\"].copy()\n",
        "final_path = OUTPUT_DIR / \"drafts.csv\"\n",
        "final_drafts.to_csv(final_path, index=False)\n",
        "\n",
        "print(\"Wrote:\")\n",
        "print(\"-\", qc_path)\n",
        "print(\"-\", final_path)\n",
        "\n",
        "# Mini-eval: pass-rate by risk_level (proxy). In class, replace with rubric scoring.\n",
        "pass_rate = (qc_report[\"risk_level\"] == \"low\").mean()\n",
        "print(f\"Proxy pass-rate (risk_level==low): {pass_rate:.2%}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extensions / Optional challenges\n",
        "\n",
        "- **Rubric-based grader**: have the model score drafts using `data/rubric.md`; compare to human scores.\n",
        "- **Batching + cost controls**: cache lead summaries; estimate token/cost; compare one-pass vs two-pass QC.\n",
        "- **Policy-driven compliance**: translate `compliance_tags` into explicit deny/allow rules and required disclaimers.\n",
        "- **Prompt/version tracking**: log prompt templates, model/version, and params alongside outputs.\n",
        "- **Multi-variant testing**: run A/B prompt variants and choose winners via eval metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
